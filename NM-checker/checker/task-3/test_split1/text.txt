function [J, grad] = cost_function(params, X, y, lambda, ...
                   input_layer_size, hidden_layer_size, ...
                   output_layer_size)
   
   theta1_size = [hidden_layer_size, input_layer_size + 1];
   theta2_size = [output_layer_size, hidden_layer_size + 1];
   
   theta1 = reshape(params(1 : prod(theta1_size)), theta1_size(1), theta1_size(2));
   theta2 = reshape(params((prod(theta1_size) + 1) : end), theta2_size(1), theta2_size(2));
   
   [rows cols] = size(X);
   %expandam y
    Y = zeros(rows, 10);
    for i = 1 : rows
        Y(i, :) = eye(10)(y(i), :);
    endfor
    
   J = 0; 
   for i = 1 : rows
       for k = 1 : 10
          J = J -Y(i, k) * log10((apply_h(X(i,:), theta1, theta2))(k,1)) - (1 - Y(i, k)) * log10(1 - (apply_h(X(i,:), theta1, theta2))(k,1));
       endfor
   endfor
   
   J = J / rows;
   
   S = 0;
   for j = 2 : (input_layer_size + 1)
       for k = 1 : hidden_layer_size
           S = S + theta1(k,j) * theta1(k,j);
       endfor
   endfor
   
   for j = 2 : (hidden_layer_size + 1)
       for k = 1 : output_layer_size
           S = S + theta2(k, j) * theta2(k, j);
       endfor
   endfor
   
   S = lambda * S / 2 / rows;
   
   J = J + S;
   grad = 0;
   
  % params -> vector containing the weights from the two matrices
  %           Theta1 and Theta2 in an unrolled form (as a column vector)
  % X -> the feature matrix containing the training examples
  % y -> a vector containing the labels (from 1 to 10) for each
  %      training example
  % lambda -> the regularization constant/parameter
  % [input|hidden|output]_layer_size -> the sizes of the three layers
  
  % J -> the cost function for the current parameters
  % grad -> a column vector with the same length as params
  % These will be used for optimization using fmincg
  
  % TODO: cost_function implementation

  % TODO1: get Theta1 and Theta2 (from params). Hint: reshape
  
  % TODO2: Forward propagation
  
  % TODO3: Compute the error in the output layer and perform backpropagation
  
  % TODO4: Determine the gradients
  
  % TODO5: Final J and grad

endfunction
